% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass{tlp} % \documentclass[runningheads]{llncs}

% \usepackage{graphicx}
\usepackage{amsmath}
% \usepackage{amssymb}
\usepackage{helvet,times,courier}
% \usepackage[inline]{enumitem}
% \usepackage{adjustbox}
% \usepackage{array}
% \newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}
% \newcolumntype{M}[1]{>{\centering\arraybackslash}m{#1}}

\usepackage{tikz}
\usetikzlibrary{positioning,patterns}
\newlength{\scaledx}
\newlength{\scaledy}
\newcommand\SetScales{%
  \pgfextractx{\scaledx}{\pgfpointxy{1}{0}}%
  \pgfextracty{\scaledy}{\pgfpointxy{0}{1}}%
}

\usepackage{calc}
\newlength\listingnumberwidth
\setlength\listingnumberwidth{\widthof{\scriptsize 00} + 10pt}
% \setlength{\mathindent}{\listingnumberwidth}

\usepackage{listings}
\lstset{numberbychapter=false,numbers=left,numberblanklines=false,basicstyle=\ttfamily\small,%xleftmargin=5ex,
xleftmargin=\listingnumberwidth,captionpos=b,breaklines=false,frame=single}

\usepackage{multirow}
\usepackage{xspace}
\usepackage{url}

\usepackage{comment}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following line
% to display URLs in blue roman font according to Springer's eBook style:
% \renewcommand\UrlFont{\color{blue}\rmfamily}

\newcommand{\clingo}{\emph{clingo}\xspace}
\newcommand{\clingodl}{\emph{clingo}[DL]\xspace}
\newcommand{\figspace}{\vspace{-1ex}}

\makeatletter
\def\fcapsize@figure{\normalfont\normalsize\rmfamily}
\def\fcapstyle@figure{\normalfont\normalsize\itshape}
\makeatother

\begin{document}

\lefttitle{Mohammed M.\ S.\ El-Kholany, Martin Gebser, Konstantin Schekotihin}

\jnlPage{1}{8}
\jnlDoiYr{2021}
\doival{10.1017/xxxxx}
%
\title{Problem Decomposition and Multi-shot ASP Solving for Job-shop Scheduling%
% \thanks{This work was partially funded by
% KWF project 28472,
% cms electronics GmbH,
% FunderMax GmbH,
% Hirsch Armb채nder GmbH,
% incubed IT GmbH,
% Infineon Technologies Austria AG,
% Isovolta AG,
% Kostwein Holding GmbH, and
% Privatstiftung K채rntner Sparkasse.}%
}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
% \author{Mohammed M. S. El-Kholany\inst{1}\orcidID{0000-0002-1088-2081} \and
% Martin Gebser\inst{1,2}\orcidID{0000-0002-8010-4752} \and
% Konstantin Schekotihin\inst{1}\orcidID{0000-0002-0286-0958}}
% %
% \authorrunning{M. El-Kholany et al.}
% % First names are abbreviated in the running head.
% % If there are more than two authors, 'et al.' is used.
% %
% \institute{Alpen-Adria-Universit채t Klagenfurt, Klagenfurt, Austria 
% \email{\{mohammed.el-kholany, martin.gebser and konstantin.schekotihin\}@aau.at}\\
% %\url{http://www.springer.com/gp/computer-science/lncs} \and
% Technische Universit채t Graz, Graz, Austria\\
% \email{mgebser@ist.tugraz.at}}
\begin{authgrp}
\author{\sn{Mohammed M.\ S.} \gn{El-Kholany}} % \inst{1}\orcidID{0000-0002-1088-2081} \and
\affiliation{University of Klagenfurt, Austria \and Cairo University, Egypt}
\author{\sn{Martin} \gn{Gebser}} % \inst{1,2}\orcidID{0000-0002-8010-4752} \and
\affiliation{University of Klagenfurt, Austria \and Graz University of Technology, Austria}
\author{\sn{Konstantin} \gn{Schekotihin}} % \inst{1}\orcidID{0000-0002-0286-0958}
\affiliation{University of Klagenfurt, Austria}
\end{authgrp}

\history{\sub{xx xx xxxx;} \rev{xx xx xxxx;} \acc{xx xx xxxx}}

\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Scheduling methods are important for effective production and logistics management, where tasks need to be allocated and performed by limited resources.
In particular, the Job-shop Scheduling Problem (JSP) is a well-known and challenging
combinatorial optimization problem in which tasks sharing a machine are to be arranged
in sequence such that encompassing jobs can be completed as early as possible.
Given that already moderately sized JSP instances can turn out as highly combinatorial,
so that neither optimal schedules nor the runtime to termination of complete optimization methods
is known, efficient approaches to approximate good-quality schedules are of interest.
In this paper, we propose problem decomposition into time windows whose operations
can be successively scheduled and optimized by means of multi-shot ASP solving.
From a computational perspective, decomposition aims to split highly complex scheduling tasks into better manageable subproblems with a balanced number of 
operations, so that good-quality or even optimal partial solutions can be
reliably found in a small fraction of runtime.
Regarding the feasibility and quality of solutions, problem decomposition must
respect the precedence of operations within their jobs, and partial schedules
optimized by time windows should yield better global solutions than obtainable
in similar runtime on the full problem.
We devise and investigate a variety of decomposition strategies in terms of the number and size of time windows as well as heuristics for choosing their operations.
Moreover, we incorporate time window overlapping and compression techniques
into the iterative scheduling process in order to counteract limitations of window-wise optimization restricted to partial schedules.
Our experiments on JSP benchmark sets of several sizes show that
successive optimization by multi-shot ASP solving leads to substantially better schedules within the runtime limit than global optimization on the full problem,
where the gap increases with the number of operations to schedule.
%
% This paper applied a decomposition approach that splits the problem into a pre-defined set of small problems called Time Windows (TWs) and solves each separately in sequential order. Our proposed model is split into two main phases for solving the (JSP). The decomposition phase assigns each operation to a particular TW while satisfying the precedence constraint. The second phase is the scheduling phase, in which the model starts to optimize the first TW and continues until finishing the last TW. We built our model using Answer Set Programming (ASP). A Multi-shot solving paradigm is used to deal with the decomposition approach and differenc logic to handle the constraints. To evaluate the proposed work, we tested our model on a set of benchmark instances with different sizes and the results showed that the multi-shot solving is better than the single-shot when the number of operations increases. 
% \keywords{Job-shop Scheduling Problem  \and Answer Set Programming \and Decomposition.}
\end{abstract}
%
\begin{keywords}
Job-shop Scheduling Problem, Answer Set Programming, Problem Decomposition
\end{keywords}
%
%
\section{Introduction}
% In today's competitive markets, manufacturers have to respond quickly to orders and meet shipping dates committed to the customers. This requires the ability to schedule production activities to use the available scarce resources efficiently. Effective scheduling techniques are essential in complex manufacturing systems such as semiconductor manufacturing. In general, scheduling operations is one of the most critical issues in the planning and managing of the manufacturing processes\cite{uzsoy2000performance}.
Effective scheduling methods are essential for complex manufacturing and logistics systems,
where allocating and performing diverse tasks within resource capacity limits 
is one of the most critical challenges for production management \citep{uzsoy2000performance}.
The Job-shop Scheduling Problem (JSP) \citep{baker1974introduction,taillard1993benchmarks}
constitutes a well-known mathematical abstraction of industrial production scheduling in
which sequences of operations need to be processed by machines such that a given objective
like the makespan for completing all jobs or their tardiness w.r.t.\ deadlines is minimized.
Finding optimal JSP solutions, determined by a sequence of operations for each machine,
is an NP-hard combinatorial problem
\citep{garey1976complexity,lenstra1977complexity,liu2008prediction},
and optimal schedules as well as termination guarantees can be extremely challenging
or beyond reach 
of complete optimization methods already for moderately sized instances.
For example, it took about $20$ years to develop a search procedure able to find a
(provably) optimal solution for an instance called FT10 with $10$ jobs
\citep{adams1988shifting,zhang2010hybrid},
each consisting of a sequence of $10$ operations to be processed by $10$ machines. 

% One of the most challenging scheduling problems is the Jop-shop Scheduling Problem (JSP). A set of jobs needs to be processed on a set of machines while optimizing a performance indicator, minimizing makespan, the time needed to complete all the jobs, or tardiness, i.e., the summation of the delays in executing all jobs according to their deadlines. Each job has a set of consecutive operations; each operation requires exactly one machine; machines are continuously available and can process only one operation at a time. The main purpose is to determine the sequence of the operations on each machine to optimize a performance indicator. The JSP is an NP-complete combinatorial optimization problem where obtaining the optimal solution is challenging regardless of the problem scale\cite{garey1976complexity,lenstra1977complexity,liu2008prediction}. For instance, benchmark (FT10) consists of $10$ jobs and $10$ machines took researchers roughly $20$ years to find the optimal solution\cite{adams1988shifting,zhang2010hybrid}.

In real-world production scheduling, the number of operations to process can easily go
into tens of thousands \citep{coltep19a,kohakamo20a,kotakoscge21a}, which exceeds exact
optimization capacities even of state-of-the-art solvers for Answer Set Programming (ASP),
Mixed Integer Programming (MIP), or Constraint Programming (CP)
\citep{daneshamooz2021mathematical,francescutto2021solving,shi2021solving}.
Hence, more efficient approaches to approximate good-quality schedules 
instead of striving for optimal solutions have attracted wide research interest.
On the one hand, respective methods include greedy and local search techniques such as
dispatching rules \citep{blackstone1982state}, shifting bottleneck \citep{adams1988shifting} and
genetic algorithms \citep{pezzella2008genetic}.
On the other hand, problem decomposition strategies based on a
rolling horizon \citep{singer2001decomposition,liu2008prediction} or
bottleneck operations \citep{zhang2010hybrid,zhai2014decomposition} 
have been proposed to partition large-scale instances into better manageable subproblems,
where no single strategy strictly dominates \citep{ovacik2012decomposition}.

% In reality, the number of operations is reaching thousands of operations. Therefore, the exact methods Answer Set Programming (ASP), Constraint Programming and Branch and Bound cannot find the optimal solutions in a reasonable time\cite{daneshamooz2021mathematical,shi2021solving,francescutto2021solving}. Since the performance of the exact methods will be hardly satisfactory, several studies have presented one of the most effective methods for solving the JSP, which is decomposition \cite{zhang2010hybrid}. The decomposition aims to split the problem into a series of subproblems based on a particular decomposition policy and then solve each part separately and obtain the final solution by integrating all of these solutions. Several strategies have been proposed to decompose the JSP, and each varies according to the problem features. More specifically, there is no evidence that one of the introduced strategies is the best for solving all JSP(s) \cite{ovacik2012decomposition}. 

While decision versions of scheduling problems can be successfully modeled and
solved by an extension of ASP with Difference Logic (DL) constraints
\citep{gebser2016theory}, implemented by \clingodl on top of the
(multi-shot) ASP system \clingo \citep{gekakasc17a},
the optimization capacities of \clingodl come to their limits on moderately sized
yet highly combinatorial JSP instances \citep{elkgeb20a}, for some of which optimal solutions are so far unknown \citep{shysha18a}.
Successful application areas beyond JSP include 
industrial printing \citep{balduccini11a},
team-building \citep{rigralmaliiile12a},
shift design \citep{abseher2016shift},
course timetabling \citep{bainkaokscsotawa18a},
lab resource allocation \citep{francescutto2021solving},
and
medical treatment planning \citep{dogagrmamopo21a},
pointing out the general attractiveness of ASP for modeling and solving scheduling problems.

In this paper, we greatly extend our preliminary study \citep{elkgeb20a} 
on problem decomposition into time windows and successive schedule optimization
by means of multi-shot ASP modulo DL solving with \clingodl.
The goal of the decomposition is to split highly complex scheduling tasks into
balanced portions for which partial schedules of good quality can be reliably
found within tight runtime limits, and then be merged into a global solution of
significantly better quality than obtainable in similar runtime 
with single-shot optimization on the full problem.
In this process, problem decomposition must respect the precedence of operations
within their encompassing jobs to guarantee the feasibility of schedules, and as
a secondary objective, the order by time windows of operations sharing a machine should come close to the sequences of operations in (unknown) optimal schedules.
We address computational efficiency as well as solution quality by devising and investigating decomposition strategies regarding the size of
time windows and heuristics to choose their operations.

The contributions of our work going beyond the study in \citep{elkgeb20a} are:
\begin{itemize}
\item In addition to problem decomposition based on the earliest starting times
      of operations, we consider the most total work remaining criterion and
      refinements of both strategies by bottleneck machines.
      We encode static as well as dynamic decomposition variants, the latter taking
      partial schedules for operations of previous time windows into account, by stratified
      ASP programs (without DL constraints).
\item Considering that a decomposition into time windows may be incompatible with
      the optimal sequences of operations sharing a machine, we incorporate
      time window overlapping into the iterative scheduling process to
      offer a chance for revising "decomposition mistakes".
      Moreover, as the makespan objective we apply for the optimization of
      (partial) schedules tolerates unnecessary idle times of machines as long as
      they do not yield a greater scheduling horizon,
      we devise a stratified ASP encoding to postprocess and compress partial
      schedules by reassigning operations to earlier idle slots available on their machines.
\item We experimentally evaluate decomposition strategies varying the number of
      operations per time window as well as multi-shot ASP solving
      augmented with overlapping and compression techniques
      on JSP benchmark sets of several sizes.
      In particular, our experiments demonstrate that successive optimization
      by multi-shot ASP solving leads to substantially better schedules within
      tight runtime limits than global optimization on the full problem,
      where the gap increases with the number of operations to schedule.
\end{itemize}

% In this study, we aim to introduce a new decomposition strategy for solving the JSP. Our study proposes a new policy to efficiently divide the problem into subproblems, aiming to reach a near-optimal solution. Then each part is solved using an ASP scheduler-based, and the solutions are integrated to obtain the solution of the whole problem. We develop and implement our model using ASP, which is considered as one of the most popular paradigms for knowledge representation and reasoning, especially in combinatorial optimization problems \cite{abseher2016shift}. 

The rest of this 
paper is organized as follows.
Section~\ref{sec:preliminaries} briefly introduces ASP along with the relevant
extensions of multi-shot solving and DL constraints.
In Section~\ref{sec:problem}, we present our successive optimization approach
and detail the ASP programs encoding problem decomposition or iterative scheduling by
time windows, respectively.
Section~\ref{sec:experiments} provides experimental results on JSP benchmark sets, % of several sizes,
assessing different decomposition strategies as well as the impact of overlapping and
compression techniques.
Conclusions and future work are discussed in Section~\ref{sec:conclusions}.  
% The following section shows the most related work and which decomposition techniques have been introduced in the past, followed by the problem formulation section illustrates the problem in detail. Section $4$ presents our proposed model, which shows the decomposition techniques we used and describes one in more detail. In addition, we present the schedule model using Answer Set Programming. Our model is tested by performing extensive experiments on a set of benchmark instances in Section $5$. Section $6$ summarizes this work and provides some ideas to extend the current work.

% \section{Literature Review}
% This section will review some of the work that has applied the decomposition approach for solving scheduling problems. As mentioned in the previous section, no particular decomposition method has proved its efficiency in solving all scheduling problems. However, many articles have introduced efficient decomposition methods for solving particular scheduling problems with specific features. The decomposition idea for solving JSP has been initially suggested by introducing a \textit{Shifting bottleneck} (SB) procedure which decomposes the problem into parts; each part contains only one machine \cite{adams1988shifting}. At each iteration, the bottleneck machine is determined from a set of unsequenced machines and then scheduled. Afterward, all the previously established sequences are locally reoptimized and iterate until all the machines have been sequenced. The idea of SB had been later improved by integrating an optimization algorithm for solving the one-machine problem with the delayed precedence constraints procedure \cite{balas1995one}.

% Another study investigated the performance of a new decomposition procedure based on SB in the Job-shop and flow shop with different levels of bottleneck machines. The computational experiments showed that the proposed method obtained better solutions in a shorter time than for the problems in which the machine's workload is identical. Other different methods have been developed to decompose the JSP. For instance, a rolling horizon heuristic has been presented \cite{singer2001decomposition} to solve a large-scale job shop. The authors have decomposed the problem and solve each independently while minimizing the total weighted tardiness. They tested their model on a set of benchmark instances, and the results showed that the proposed model is superior to large instances. 

% The rolling horizon procedure has been extended by constructing a prediction model to obtain the scheduling characteristics values, including the information of the bottleneck jobs. The obtained information aided in decomposing the problem efficiently. In addition, they have proposed a genetic algorithm to solve each sub-problem. In order to evaluate the performance of the proposed model, they performed numerical computational experiments that showed the effectiveness of the model \cite{liu2008prediction}. A decomposition-based hybrid optimization algorithm has been introduced for the JSP, where the total weighted tardiness is minimized. A Simulated Annealing is used to define subproblems iteratively and then is solved by a Genetic Algorithm. They have developed a fuzzy system that provides information about bottleneck jobs. This information is used to guide the process of subproblem-solving to promote the optimization efficiency \cite{zhang2010hybrid}. The numerical computational results showed that the proposed algorithm is effective for large-scale scheduling problems.

% A decomposition method based on bottleneck machines has been presented for solving the JSP. The proposed model decomposes the problems into subproblems and then detects the multi-bottleneck machines using a critical path method. The information of the bottleneck machines has been employed to improve the solution quality by splitting the operations into bottleneck operations and non-bottleneck operations, where the bottleneck operations are scheduled by the Genetic Algorithm and the non-bottleneck operations are scheduled by dispatching rules \cite{zhai2014decomposition}.

% From the literature, we can find that most of the related work focused on applying the decomposition approach while minimizing the total tardiness, and the number of work tackled large-scale instances to minimize the makespan is relatively low. This paper proposes different decomposition strategies for solving the JSP with larger instances using the ASP model while minimizing the makespan.

\section{Preliminaries}\label{sec:preliminaries}
\paragraph{Answer Set Programming (ASP)} is a declarative programming language for solving hard combinatorial optimization problems. ASP has become an established paradigm for knowledge representation and reasoning. ASP has proved its efficiency for solving the combinatorial optimization problem in different applications such as bioinformatics\cite{erdem2015generating,koponen2015optimizing}, databses\cite{caniupan2010consistency} and scheduling and industrial applications~\cite{dodaro2015allotment,dodaro2016combining,fabricius2020towards,dodaro2021operating}.

A logic program is a finite set of rules of the form 
\begin{equation}
	a_0 \gets a_1, \ldots, a_m, \sim a_{m+1}, \dots, \sim a_n
	\label{rule1}
\end{equation}
where $a_i$ is an atom for $0 \leq i \leq n$ and ``$\sim$''  is a default negation. If $n$ quals to $0$, then a rule \eqref{rule1} is a fact. If $a_0$ is omitted, the rule is an integrity constraint. An \emph{atom} is an expression of the form $p(t_1,\ldots,t_l)$, where $p$ is a predicate and $t_1,\ldots,t_l$ are \emph{terms}. Each term can be a variable or a constant. A \emph{literal} is either an atom or its negation. 
Given a rule $r$ of the form \eqref{rule1}, the set $H(r)=a_0$ denotes the \emph{head} atom and the set $B(r) = B^+(r) \cup B^-(r) = \{a_1,\dots,a_m\} \cup \{\sim a_{m+1}, \dots \sim a_n\}$ is the \emph{body} of $r$ where $B^+(r)$ and $B^+(r)$ represent the positive and negative body literals, respectively.
\paragraph{Multi-shot Solving:}
Multi-shot solving ASP allows solving logic programs dynamically in sequential order. This can be manipulated using APIs implementation via an imperative programming language. Such programming language is used to control the grounding and solving processes and allows the usage of external variables that could be set to $True$ or $False$ to control some logic rules. The main advantage of \emph{multi-shot} solving is to avoid the re-grounding and exploit conflicts learned over time.
%ASP is a paradigm that deals with continuously changing logic programs. In a single-shot approach, an ASP system takes a logic program, computes answer sets and exits. However, the idea of Multi-shot is to consider evolving grounding and solving processes. \emph{clingo} systems enhances the ASP declaritive language with control capacities. 
\paragraph{ASP Difference Logic:}
\emph{clingo}[DL] is an extension of the input language \emph{clingo} by theory atoms representing difference constraints \cite{gebser2016theory,janhunen2017clingo}. Difference constraints are defined by specific constraint atoms of the form $\text{\lstinline{&diff}}\{u-v\} \leq k$ where $u$ and $v$ are terms which are interpreted as integer variables and $k$ is an integer constant. \emph{clingo}[DL] provides the following extension of the normal form \eqref{rule1}: 
\begin{align*}
	\text{\lstinline{&diff}}\{u-v\} \leq k \gets a_1, \ldots, a_m, \sim a_{m+1}, \ldots, \sim a_n.
\end{align*}
 This rule shows that whenever the body is $True$ the inequality constraints must be satisfied.

\section{Multi-shot JSP Solving}\label{sec:problem}
This section describes our successive optimization approach to
JSP solving by means of multi-shot ASP with \clingodl.
We start with specifying the fact format for JSP instances,
then detail problem decomposition based on earliest starting times of operations,
present our ASP encoding with DL constraints for optimizing the makespan of partial schedules, and
finally outline the incorporation of time window overlapping and compression
into the iterative scheduling process.   
% The Job-shop Scheduling Problem (JSP) has been a complex and combinatorial optimization problem since the 1950s, and it was shown to be NP-hard. In the JSP, there are a set of jobs to be processed on a set of machines. The number of jobs is $n$, and the number of machines is $m$. Each job $J_i$ contains a chain $ O_{i,1}, O_{i,2},...,O_{i,m} $ of operations must be executed on an known order. The routings of the operations are deterministic and known a priori, as are the processing times of each operation on each machine. We consider here the problem of minimizing the total completion time (makespan). The task of the scheduling is to determine the starting time for each operation while optimizing the makespan. The basic assumptions are as follows:
%
% \begin{enumerate}
% 	\item The processing time of each operation is fixed.
% 	\item Machine breakdown preemption of the operations are not allowed.
% 	\item Each machine can process only one operation at a time.
% 	\item Each operation can be executed by only one machine.
% 	\item A machine cannot process more than one operation at a time.
% 	\item The jobs and the machines are available at time $0$.
% \end{enumerate}

\subsection{Problem Instance}\label{subsec:instance}
% ASP has been applied to solve different scheduling problems. For example, ASP is used to solve scheduling problems in the healthcare systems \cite{dodaro2017nurse,dodaro2019asp}. One of these studies aimed to assign nurses to shifts according to some constraints\cite{dodaro2017nurse}. In addition, ASP has been proposed to solve a train scheduling problem while minimizing the train delay\cite{abels2019train}. One of the limitations of ASP mentioned in these studies is the grounding issues while solving a large number of instances. One of the recent works has presented a multi-shot solving for scheduling problem \cite{francescutto2021solving}. This paper aims to use different decomposition methods to solve the JSP using ASP with Difference Logic. 
% 
% \paragraph{Problem Instances.} 
%
Each job in a JSP instance is a sequence of operations with associated
machines and processing times.
Corresponding facts for an example instance with three jobs and three
machines are displayed in Listing~\ref{prg:facts}.
An atom of the form
\lstinline{operation(}$j$\lstinline{,}$s$\lstinline{,}$m$\lstinline{,}$p$\lstinline{)}
denotes that the step~$s$ of job~$j$ needs to be processed by machine~$m$ for $p$ time units.
For example, the second operation of job~\lstinline{3} has a processing time of
\lstinline{3} time units on machine~\lstinline{1},
as specified by the fact
\lstinline{operation(3,}\linebreak[1]\lstinline{2,}\linebreak[1]\lstinline{1,3).}
The operation cannot be performed before the first operation of job~\lstinline{3}
is completed,
and its execution must not intersect with the first operation of job~\lstinline{1}
or the second operation of job~\lstinline{2},
which need to be processed by machine~\lstinline{1} as well.
That is, a schedule for the example instance must determine a sequence in which
to process the three mentioned operations on machine~\lstinline{1},
and likewise for operations sharing machine~\lstinline{2} or~\lstinline{3}, respectively.
%
% In order to encode the problem using ASP, we should define a set of predicates that represents the problem instances. For instance, the operations are encoded with the predicate \lstinline{operation/2} where the first variable denotes the job number and the second is the operation number, see lines \ref{prg:facts:ops:begin}-\ref{prg:facts:ops:end} in Listing \ref{prg:facts}. The time needed to finish an operation is represented by the predicate \lstinline{pro/3} in which the operation is determined by the first two terms and the third represents the processing time in lines \ref{prg:facts:pro:begin}-\ref{prg:facts:pro:end}. For the machine assignment, the predicate \lstinline{assign/3} shows that a particular operation is executed by a machine in lines \ref{prg:facts:mach:begin}-\ref{prg:facts:mach:end}. For instance, \emph{operation(1,1)} is processed by machine 2. 
\lstinputlisting[float=bt,mathescape=true,escapeinside={\#(}{\#)},basicstyle={\ttfamily\small},label=prg:facts,caption={Example JSP instance},linerange={1-3}]{listing/facts.lp}

Figure~\ref{fig:schedule} depicts an optimal schedule in terms of makespan,
i.e., the latest completion time of any job/operation,
for the JSP instance from Listing~\ref{prg:facts}.
The \textbf{J}-\textbf{S} pairs in horizontal bars indicated for the
machines~\lstinline{1}, \lstinline{2}, and~\lstinline{3} identify operations
by their job~\textbf{J} and step number~\textbf{S}.
For each machine, observe that the bars for operations it processes do not
intersect, so that the operations are performed in sequential order.
Moreover, operations belonging to the same job are scheduled one after the other.
For example,
the second operation of job~\lstinline{3} is only started after the
completion of the predecessor operation at time~$9$,
regardless of the availability of its machine~\lstinline{1}
from time~$3$ on.
As the precedence of operations within their jobs must be respected
and the sum of processing times for operations of job~\lstinline{3}
matches the makespan~$20$, it is impossible to reduce the scheduling
horizon any further,
which in turn means that the schedule shown in Figure~\ref{fig:schedule} is optimal.
%
\begin{figure}[t]
\begin{tikzpicture}[x=3.5ex,y=3.5ex,label distance=-1ex,thick]
\SetScales
\foreach \i in {1,...,3} {
  \node[label=left:Machine \i] at (0,3.5-\i) {};
}
\foreach \i in {0,...,20} {
  \draw[dotted] (\i,0) -- (\i,3) node [below] at (\i,0) {$\i$};
}
\foreach \i in {0,...,3} {
  \draw[dotted] (0,\i) -- (20,\i);
}
\node[minimum width=3\scaledx-0.05\scaledx,minimum height=1\scaledy-0.05\scaledy,inner sep=0,rectangle,draw,anchor=south west,pattern color=lightgray,pattern=north west lines] at (0,2) {\textbf{1}-\textbf{1}};
\node[minimum width=3\scaledx-0.05\scaledx,minimum height=1\scaledy-0.05\scaledy,inner sep=0,rectangle,draw,anchor=south west,pattern color=lightgray,pattern=crosshatch] at (9,2) {\textbf{3}-\textbf{2}};
\node[minimum width=6\scaledx-0.05\scaledx,minimum height=1\scaledy-0.05\scaledy,inner sep=0,rectangle,draw,anchor=south west,pattern color=lightgray,pattern=grid] at (12,2) {\textbf{2}-\textbf{2}};
\node[minimum width=4\scaledx-0.05\scaledx,minimum height=1\scaledy-0.05\scaledy,inner sep=0,rectangle,draw,anchor=south west,pattern color=lightgray,pattern=grid] at (0,1) {\textbf{2}-\textbf{1}};
\node[minimum width=3\scaledx-0.05\scaledx,minimum height=1\scaledy-0.05\scaledy,inner sep=0,rectangle,draw,anchor=south west,pattern color=lightgray,pattern=north west lines] at (4,1) {\textbf{1}-\textbf{2}};
\node[minimum width=8\scaledx-0.05\scaledx,minimum height=1\scaledy-0.05\scaledy,inner sep=0,rectangle,draw,anchor=south west,pattern color=lightgray,pattern=crosshatch] at (12,1) {\textbf{3}-\textbf{3}};
\node[minimum width=9\scaledx-0.05\scaledx,minimum height=1\scaledy-0.05\scaledy,inner sep=0,rectangle,draw,anchor=south west,pattern color=lightgray,pattern=crosshatch] at (0,0) {\textbf{3}-\textbf{1}};
\node[minimum width=1\scaledx-0.05\scaledx,minimum height=1\scaledy-0.05\scaledy,inner sep=0,rectangle,draw,anchor=south west,pattern color=lightgray,pattern=north west lines] at (9,0) {\textbf{1}-\textbf{3}};
\node[minimum width=2\scaledx-0.05\scaledx,minimum height=1\scaledy-0.05\scaledy,inner sep=0,rectangle,draw,anchor=south west,pattern color=lightgray,pattern=grid] at (18,0) {\textbf{2}-\textbf{3}};
\end{tikzpicture}
\figspace
\caption{Optimal schedule for example JSP instance\label{fig:schedule}}% in Listing~\ref{prg:facts}\label{fig:schedule}}
\end{figure}

\subsection{Problem Decomposition}\label{subsec:decomposition}
As we mentioned in the previous sections, it is hard to solve the JSP and reach a near-optimal solution in a reasonable time, especially when the number of operations to be processed is high. The decomposition approach has been widely used to solve the JSP by many researchers and proved its efficiency.
%
\lstinputlisting[float=bt,mathescape=true,escapeinside={\#(}{\#)},basicstyle={\ttfamily\small},label=prg:decomposition,caption={Time-based EST decomposition}]{listing/decomposition.lp}

This section will mention $4$ different decomposition strategies that we developed to split the problem into parts, describe one of them in detail, and show the encoding. The decomposition strategies can be classified into two categories; time-based and machine-based. The time-based decomposition aims to prioritize the operations based on the processing time. However, machine-based decomposition methods consider the processing time of the operations and the machine's workload. The main idea behind the decomposition procedure is to find a criterion that ranks the operations to assign in the proper time window to obtain higher-quality schedules without violating the precedence constraints. Earliest Starting Time (EST) and Most Total Work Remaining (MTWR) procedures have been applied to rank the order the operations. The decomposition procedures will be shown as follows:

\paragraph{\textbf{EST Time-based}:} it ranks the operations based on calculating the earliest possible starting time for each operation. It is calculated by aggregating its predecessor(s) processing time. The operation with smaller EST will be assigned to a time window before the others with greater EST.

\paragraph{\textbf{MTWR Time-based}:} the operation rank is determined based on the work remaining of a job to be completed. The operation belongs to a job with a high remaining processing time, will be assigned earlier to a time window.

\paragraph{\textbf{EST Machine-based}:} it applies the same idea as EST Time-based. However, the bottleneck machine is taken into account throughout the decomposition process. More specifically, the operation with smaller EST and executed by a bottleneck machine will be assigned earlier to a time window.

\paragraph{\textbf{MTWR Machine-based}:} we calculate the MTWR of each operation, and the operation with the Largest MTWR and processed by a bottleneck machine will be assigned earlier to a time window.

In this study, we will describe in detail the decomposition of the operations based on the \textbf{EST Time-based}. We have split the decomposition encoding into two parts; the first part is for the pre-decomposition phase, which is in Listing \ref{prg:predeco}. The first two lines are to determine the number of time windows, which is two in this example. The lines \ref{prg:predeco:jmo:begin}-\ref{prg:predeco:jmo:end} are to calculate the total number of jobs and machines of a particular instance, respectively. Therefore, the total number of operations to be scheduled is computed by the rule in lines \ref{prg:predeco:o:begin}-\ref{prg:predeco:o:end}. In line \ref{prg:predeco:optw:begin}, the number of operations assigned to a time window is determined. 

\lstinputlisting[float=bt,mathescape=true,escapeinside={\#(}{\#)},basicstyle={\ttfamily\small},label=prg:predeco,caption={Pre-decomposition}, linerange={1-15} ]{listing/pre_deco.lp}
Listing \ref{prg:deco} shows the decomposition using \emph{EST Time-based} strategy. The first two rules in the lines \ref{prg:assignment_process:est:begin}-\ref{prg:assignment_process:est:end} calculate the earliest possible start time of each operation, where the EST for the first operation of all jobs are $0$ in line \ref{prg:assignment_process:est:begin}. The second rule calculates the other operations by aggregating the processing time of their predecessors. 

We prioritize the operations based on the estimated starting time, calculating an index for each operation. The operation with a shorter estimated start time will get a smaller index than the others with a higher estimated starting time. If the estimated starting time between two operations or more is similar, we check the processing time; the higher the processing time, the smaller index, and, therefore, the higher priority to be assigned earlier. If the processing time is equal, we look at the operation number and then the job number. This strategy is encoded in lines \ref{prg:assignment_process:rank:begin}-\ref{prg:assignment_process:rank:end}. The fourth rule in lines \ref{prg:assignment_process:assign:begin}-\ref{prg:assignment_process:assign:end} is to assign each operation to a time window.

\lstinputlisting[float=bt,mathescape=true,escapeinside={\#(}{\#)},basicstyle={\ttfamily\small},label=prg:deco,caption={EST-decomposition}, linerange={1-20} ]{listing/assignment_process.lp}

After running the decomposition encoding, we described above; we get an assignment of each operation to a TW. This assignment is represented by a set of atoms shown in the Listing \ref{prg:tw}. We can see that the operations operations $\{ O_{1,1}, O_{2,1}, O_{2,2}, O_{3,1}, O_{3,2} \}$ are assigned to the first TW and the rest to the second TW.

\lstinputlisting[float=bt,mathescape=true,escapeinside={\#(}{\#)},basicstyle={\ttfamily\small},label=prg:tw,caption={Time Window Assignment}, linerange={1-7} ]{listing/TW.lp}

\subsection{Multi-shot with JSP}
This section will show how to solve the problem dynamically, where the output of each iteration is an input to the next and merging them to obtain the solution of the whole problem. In the first step, we consider the output of the decomposition phase, which is the assignment of the operations to TW as facts. For the multi-shot solving, the encoding is split into two parts; the base, which is run and grounded one time at the beginning of the optimization process presented in Listing \ref{prg:base}. The first rule is to determine the logical sequence between operations of same job in lines \ref{prg:base:same_job:begin}-\ref{prg:base:same_job:end} which is represented in predicate \lstinline{seqL/3}. On the other hand, Lines \ref{prg:base:sharing_machine:begin}-\ref{prg:base:sharing_machine:end} identify the sequence of the operations assigned to the same machine with predicate \lstinline{sameMach/4}.

\lstinputlisting[float=bt,mathescape=true,escapeinside={\#(}{\#)},basicstyle={\ttfamily\small},label=prg:base,caption={base-prog} ]{listing/base.lp}

The next iteration is to optimize each TW by solving the subproblem(t) where $t$ refers to the current TW we aim to optimize. The model starts to schedule and optimize the first TW and after a fixed amount of time, the solver stops and obtain the makespan of the current TW and the execution time of each operation represented by an atom \lstinline{startTime((Job, Step), ST, t)} where the first term is an operation, the second is the starting time and the last is the current time window. The solver moves to the next TW, where the starting time of the scheduled operations obtained from the first TW is sent entirely as an input to the next TW. Listing \ref{prg:sub} handles the sequence between the operations in different cases. The first rule in lines \ref{prg:sub_pro:logic_seq:begin}-\ref{prg:sub_pro:logic_seq:end} derives a new atom with a predicate \lstinline{seq/4} represents the logical sequence between two operations of a same job. In this rule, the first term of the head is the predecessor of a particular operation followed by the successor, the third term defines the minimum waiting time between the starting of the both, and the last term defines the TW of the successor. The second and third rules handle the case of two operations assigned to the same machine and in the same TW. On the other hand, if two operations are assigned to the same machine and different TW, the fourth and fifth rules ensure that the operation assigned to the earlier TW will be executed before the other is assigned to the subsequent TW. For instance, the fourth rule assumes that the \lstinline{operation (Job1, Step1)} is assigned to an earlier TW than \lstinline{operation (Job2, Step2)}.

% \lstinputlisting[float=bt,mathescape=true,escapeinside={\#(}{\#)},basicstyle={\ttfamily\small},label=prg:sub,caption={sub-prog}, linerange={1-40} ]{listing/sub_pro.lp}

The third part of the scheduler is shown in Listing \ref{prg:diff} that handles the difference constraints between the operations. The first rule in lines \ref{prg:diff_log:non_nega:begin}-\ref{prg:diff_log:non_nega:end} ensures that all the operations will be processed starting from time $0$. The second and the third rule fix the execution time of the operations scheduled in the previous TW. The fourth rule aimed to avoid overlapping the operations that either belongs to the same job or are assigned to the same machine. The fifth rule ensures that the ending time of all operations will not exceed the bound variable, which is the makespan in our case. 
\lstinputlisting[float=bt,mathescape=true,escapeinside={\#(}{\#)},basicstyle={\ttfamily\small},label=prg:diff,caption={diff-log}]{listing/diff_log.lp}

\section{Experiments}\label{sec:experiments}
This section will show the computational experiments that we conducted to test the performance of our proposed model. In order to evaluate our model, we performed several experiments on a set of benchmark instances with a different number of jobs and machines. The benchmark instances we considered are generated in 1993 by E. Taillard \cite{taillard1993benchmarks}, and the size of the problems variate starting from $6$ jobs and $6$ machines till $100$ jobs and $20$ machines. We have decided to focus on $3$ different sizes of the problem, which are $50$ jobs, $15$ machines, $50$ jobs and $20$ machines and $100$ jobs and $20$ machines. Our experiments had been conducted on a laptop with Intel(R) Core(TM) i7-8650U CPU @ 1.90GHz 2.11 GHz, 8th generation, and $16$ GB. We split the problem into a different number of TWs starting from $2$ - $10$ throughout each size of the instances we focused on to investigate the most appropriate number of TWs. We run the model with a fixed amount of time $1000$ seconds. We set this amount because we observed that increasing the time more did not provide better results, and when we reduced it, the results became worse. The next table shows the obtained results with solving $50 \times 15$.

\begin{comment}
\begin{table}[t]
    \caption{Results for three sizes of job shop scheduling instances with different numbers of time windows using static machine-based EST decomposition \label{tab:Table01}}%
%    \mbox{~}\hfill
%    \resizebox{0.8\textwidth}{!}{%
%    \begin{minipage}{\textwidth}
    \centering
    \begin{tabular}{l l r r r r r r r r r r}
    \hline
%       & {}        & \multicolumn{6}{c}{Group of Instances}  \\
    &  &  \multicolumn{10}{c}{Number of Time Windows} \\
    Instances & Comparison & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\
                     
    \hline\\[-2.75mm]
    {50$\times$15}       & Makespan             & $3542.1$   & $3149.7$	     &  \boldmath{$3083.8$} & $3110.7$ & $3213.6$ & $3225.3$ & $3350.5$ & $3378.5$ & $3495.0$ & $3524.9$ \\
    & CPU(Sec)             & $1000.0$	& $1000.0$   & $1000.0$	 & $934.4$ & $615.5$  & $243.8$  & $146.7$ 	& $81.0$ & $45.6$ & $18.5$  \\
    & Interrupted Calls    & $1.0$	    & $2.0$      & $3.0$	& $3.6$  & $2.7$  & $1.0$  &  $0.6$  & $0.3$ & $0.1$ & $0.1$       \\[1.5mm]
    
    
    {50$\times$20}       & Makespan             & $3879.4$   & $3479.4$	      & $3290.5$  &  \boldmath{$3260.3$} &  $3355.5$ & $3414.8$ & $3540.3$  &  $3561.5$  & $3720.7$  & $3793.1$ \\
    & CPU(Sec)             & $1000.0$	    & $1000.0$	   & $1000.0$	  & $944.9$  & $583.4$  & $321.5$  &  $161.4$  & $54.9$  & $38.0$  & $33.3$     \\
    & Interrupted Calls    & $1.0$	    & $2.0$      & $3.0$	& $3.5$ & $2.5$  & $1.6$  & $0.9$  & $0.2$ & $0.2$  & $0.1$      \\[1.5mm]
    
    {100$\times$20}      & Makespan             & $46786.1$  &  $23977.5$  & $7201.5$ &  $6288.7$                & $6106.1$  &  \boldmath{$6031.9$} & $6094.1$ & $6053.1$ & $6095.9$  & $6234.0$  \\
    & CPU(Sec) & $1000.0$	    & $1000.0$	   & $1000.0$	   & $1000.0$	    & $1000.0$	& $1000.0$	& $1000.0$ & $1000.0$	    & $1000.0$	& $1000.0$            \\
    & Interrupted Calls    & $1.0$	    & $2.0$      & $3.0$	& $4.0$ & $5.0$  & $6.0$ & $7.0$ & $8.0$ & $8.2$ & $7.9$      \\[1.5mm]

                     
                     
    \\[-2.75mm]
    \hline
    \end{tabular}
   % }
%    \end{minipage}
% }
%    \hfill\hfill\mbox{~}
\end{table}}

\end{comment}

\begin{table}[t]
    \caption{Results for three sizes of job shop scheduling instances with different numbers of time windows using static machine-based EST decomposition \label{tab:Table01}}%
%    \mbox{~}\hfill
%    \resizebox{0.8\textwidth}{!}{%
%    \begin{minipage}{\textwidth}
    \centering
    \begin{tabular}{l l r r r r r r r}
    \hline
%       & {}        & \multicolumn{6}{c}{Group of Instances}  \\
    &  &  \multicolumn{7}{c}{Number of Time Windows} \\
    Instances & Comparison & 1 & 2 & 3 & 4 & 5 & 6 & 10 \\
                     
    \hline\\[-2.75mm]
    
    {50$\times$15}       & Makespan             & $3542.1$   & $3149.7$	     &  \boldmath{$3083.8$} & $3110.7$ & $3213.6$ & $3225.3$ & $3524.9$ \\
    & CPU(Sec)             & $1000.0$	& $1000.0$   & $1000.0$	 & $934.4$ & $615.5$  & $243.8$  & $18.5$  \\
    & Interrupted Calls    & $1.0$	    & $2.0$      & $3.0$	& $3.6$  & $2.7$  & $1.0$  & $0.1$       \\[1.5mm]
    

    {50$\times$20}       & Makespan             & $3879.4$   & $3479.4$	      & $3290.5$  &  \boldmath{$3260.3$} &  $3355.5$ & $3414.8$ & $3793.1$ \\
    & CPU(Sec)             & $1000.0$	    & $1000.0$	   & $1000.0$	  & $944.9$  & $583.4$  & $321.5$   & $33.3$     \\
    & Interrupted Calls    & $1.0$	    & $2.0$      & $3.0$	& $3.5$ & $2.5$  & $1.6$  & $0.1$      \\[1.5mm]

    {100$\times$20}      & Makespan             & $46786.1$  &  $23977.5$  & $7201.5$ &  $6288.7$                & $6106.1$  &  \boldmath{$6031.9$}   & $6234.0$  \\
    & CPU(Sec) & $1000.0$	    & $1000.0$	   & $1000.0$	   & $1000.0$	    & $1000.0$	& $1000.0$	& $861.8$            \\
    & Interrupted Calls    & $1.0$	    & $2.0$      & $3.0$	& $4.0$ & $5.0$  & $6.0$ & $7.9$      \\[1.5mm]
                     
    \\[-2.75mm]
    \hline
    \end{tabular}
   % }
%    \end{minipage}
% }
%    \hfill\hfill\mbox{~}
\end{table}


\begin{table}[t]
    \caption{Comparing different static/dynamic decomposition strategies \label{tab:Table02}}%
%    \mbox{~}\hfill
%    \resizebox{0.8\textwidth}{!}{%
%    \begin{minipage}{\textwidth}
    \centering
    \begin{tabular}{c c c c c c c c c c}
    \hline
  
\multirow{2}*{Strategy} & \multicolumn{3}{c}{50$\times$15} & \multicolumn{3}{c}{50$\times$20} & \multicolumn{3}{c}{100$\times$20}\\
    & Makespan & Time & IC & Makespan & Time & IC & Makespan & Time & IC \\
    \hline\\[-2.75mm]
    M-Based EST (S)             & $3083.8$  & $1000.0$   & $3.0$	& \boldmath{$3260.3$}      & $944.9$ & $3.5$ & $6031.9$ & $1000.0$ & $6.0$\\
    [1.5mm]

    T-based EST (S)            & $3524.8$ & $1000.0$   & $2.0$	   & $3540.8$      & $1000.0$ & $2.0$ & $6964.4$ & $1000.0$ & $4.0$\\ 
    [1.5mm]
    
    M-based MTWR (S)        & \boldmath{$3060.3$}  & $1000.0$   & $3.0$	& $3287.9$      & $986.1$ & $3.9$ & \boldmath{$5948.6$} & $1000.0$ & $7.0$\\ 
    [1.5mm]
    
    T-based MTWR (S)            & $3456.2$  & $1000.0$   & $2.0$	& $3561.0$      & $1000.0$ & $2.0$ & $7149.6$ & $1000.0$ & $4.0$\\ 
    [1.5mm]

    M-based EST (D)            & $3098.4$  & $1000.0$   & $3.0$	& $3309.5$     & $1000.0$ & $3.3$ & $6115.1$ & $1000.0$ & $6.9$\\ 
    [1.5mm]

    \\[-2.75mm]
    \hline
    
    
    
    \end{tabular}
   % }
%    \end{minipage}
% }
%    \hfill\hfill\mbox{~}
\end{table}



\begin{table}[t]
    \caption{Comparing results of best decomposition strategy with/without dompression and overlapping \label{tab:Table03}}%
%    \mbox{~}\hfill
%    \resizebox{0.8\textwidth}{!}{%
%    \begin{minipage}{\textwidth}
    \centering
    \begin{tabular}{c c c c c c c c c c}

    \hline
    \multicolumn{10}{c}{\textbf{Without Compressing Time Windows}}\\        
\multirow{2}*{Overlap} & \multicolumn{3}{c}{50$\times$15} & \multicolumn{3}{c}{50$\times$20} & \multicolumn{3}{c}{100$\times$20}\\
    & Makespan & CPU(Sec) & IC & Makespan & CPU(Sec) & IC & Makespan & CPU(Sec) & IC \\
    \hline\\[-2.75mm]
    0\%              & $3083.8$  & $1000.0$   & $3.0$	& $3260.3$      & $944.9$ & $3.5$ & $6031.9$ & $1000.0$ & $6.0$\\
    [1.5mm]
                     
    10\%             & $3050.0$  & $1000.0$   & $3.0$	& \boldmath{$3194.9$}     & $1000.0$ & $4.0$ & $5979.9$ & $1000.0$ & $6.0$\\ 
    
    20\%             & \boldmath{$3046.0$} & $1000.0$   & $3.0$	   & $3200.6$      & $1000.0$ & $4.0$ & \boldmath{$5954.3$} & $1000.0$ & $6.0$\\ 
    
    30\%             & $3051.2$  & $1000.0$   & $3.0$	& $3451.8$      & $1000.0$ & $4.0$ & $6933.3$ & $1000.0$ & $6.0$\\ 
                     [1.5mm]
                     
    \\[-2.75mm]
    
    \multicolumn{10}{c}{\textbf{Compressing Time Windows}}\\
    \hline
    \multirow{2}*{Overlap} & \multicolumn{3}{c}{50$\times$15} & \multicolumn{3}{c}{50$\times$20} & \multicolumn{3}{c}{100$\times$20}\\
    & Makespan & CPU(Sec) & IC & Makespan & CPU(Sec) & IC & Makespan & CPU(Sec) & IC \\
    \hline\\[-2.75mm]
    0\%              & $3013.3$  & $572.3$   & $3.7$	   & $3186.8$      & $540.8$ & $3.3$ & $5713.0$ & $589.2$ & $8.6$\\
    [1.5mm]
                     
    10\%             & $2970.2$  & $600.0$   & $4.0$	   & $3149.8$      & $600.0$ & $4.0$ & $5656.0$ & $600.0$ & $9.0$\\ 
    
    20\%             & $2952.2$  & $600.0$   & $4.0$	   & \boldmath{$3147.7$}      & $600.0$ & $4.0$ & \boldmath{$5619.2$} & $600.0$ & $9.0$\\ 
    
    30\%             & \boldmath{$2943.1$}  & $600.0$   & $4.0$	   & $3162.2$      & $600.0$ & $4.0$ & $5857.0$ & $600.0$ & $9.0$\\ 
                     [1.5mm]
    \hline
    \end{tabular}
   % }
%    \end{minipage}
% }
%    \hfill\hfill\mbox{~}
\end{table}

\section{Conclusions}\label{sec:conclusions}


%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{mybibliography}
%
\bibliographystyle{tlplike} % \bibliographystyle{splncs04}
\bibliography{Ref}

\end{document}
